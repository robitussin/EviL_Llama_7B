{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import List, Union\n",
    "\n",
    "import fire\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "import os.path as osp\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unused imports removed\n",
    "from utils import fsdp_auto_wrap_policy\n",
    "from transformers import (\n",
    "    LlamaForCausalLM,\n",
    "    LlamaTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    default_data_collator,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Unused imports removed\n",
    "from utils.train_utils import (\n",
    "    set_tokenizer_params,\n",
    "    train,\n",
    "    evaluation,\n",
    "    freeze_transformer_layers,\n",
    "    check_frozen_layers_peft_model,\n",
    "    setup,\n",
    "    setup_environ_flags,\n",
    "    cleanup,\n",
    "    clear_gpu_cache,\n",
    "    get_parameter_dtypes,\n",
    "    print_model_size,\n",
    "    get_policies  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset_utils import get_preprocessed_dataset\n",
    "\n",
    "from utils.config_utils import (\n",
    "    update_config,\n",
    "    generate_peft_config,\n",
    "    generate_dataset_config,\n",
    ")\n",
    "from peft import get_peft_model, TaskType, prepare_model_for_int8_training\n",
    "import configs\n",
    "from torch.distributed.fsdp import (\n",
    "    FullyShardedDataParallel as FSDP,\n",
    "    MixedPrecision,\n",
    ")\n",
    "from torch.utils.data import DistributedSampler\n",
    "import policies\n",
    "from policies import AnyPrecisionAdamW\n",
    "from configs import fsdp_config, train_config\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from pkg_resources import packaging\n",
    "import torch\n",
    "import torch.cuda.nccl as nccl\n",
    "import torch.distributed as dist\n",
    "from transformers.models.llama.modeling_llama import LlamaDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(**kwargs):\n",
    "    # Update the configuration for the training and sharding process\n",
    "    update_config((train_config, fsdp_config), **kwargs)\n",
    "\n",
    "    # Set the seeds for reproducibility\n",
    "    torch.cuda.manual_seed(train_config.seed)\n",
    "    torch.manual_seed(train_config.seed)\n",
    "\n",
    "    if train_config.enable_fsdp:\n",
    "        setup()\n",
    "        # torchrun specific\n",
    "        local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "        rank = int(os.environ[\"RANK\"])\n",
    "        world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "\n",
    "    if torch.distributed.is_initialized():\n",
    "        torch.cuda.set_device(rank)\n",
    "        setup_environ_flags(rank)\n",
    "    \n",
    "    # Calculate gradient accumulation steps\n",
    "    gradient_accumulation_steps = train_config.batch_size_training // train_config.micro_batch_size\n",
    "     \n",
    "    # Load the pre-trained model and setup its configuration\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        train_config.model_name,\n",
    "        load_in_8bit=True if train_config.quantization else None,\n",
    "        device_map=\"auto\" if train_config.quantization else None,\n",
    "    )\n",
    "    \n",
    "    print_model_size(model, train_config, rank if train_config.enable_fsdp else 0)\n",
    "    \n",
    "    # Prepare the model for int8 training if quantization is enabled\n",
    "    if train_config.quantization:\n",
    "        model = prepare_model_for_int8_training(model)\n",
    "        \n",
    "    # Convert the model to bfloat16 if fsdp and pure_bf16 is enabled\n",
    "    if train_config.enable_fsdp and fsdp_config.pure_bf16:\n",
    "        model.to(torch.bfloat16)\n",
    "\n",
    "    # Load the tokenizer and add special tokens\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(train_config.model_name)\n",
    "    tokenizer.add_special_tokens(\n",
    "            {\n",
    "            \n",
    "                \"pad_token\": \"<PAD>\",\n",
    "            }\n",
    "        )\n",
    "    if train_config.use_peft:\n",
    "        peft_config = generate_peft_config(train_config, kwargs)\n",
    "        model = get_peft_model(model, peft_config)\n",
    "        model.print_trainable_parameters()\n",
    "    \n",
    "    #setting up FSDP if enable_fsdp is enabled\n",
    "    if train_config.enable_fsdp:\n",
    "        if not train_config.use_peft and train_config.freeze_layers:\n",
    "            \n",
    "            freeze_transformer_layers(train_config.num_freeze_layers)\n",
    "\n",
    "        mixed_precision_policy, wrapping_policy = get_policies(fsdp_config, rank)\n",
    "        my_auto_wrapping_policy = fsdp_auto_wrap_policy(model, LlamaDecoderLayer)\n",
    "   \n",
    "        model = FSDP(\n",
    "            model,\n",
    "            auto_wrap_policy= my_auto_wrapping_policy if train_config.use_peft else wrapping_policy,\n",
    "            mixed_precision=mixed_precision_policy if not fsdp_config.pure_bf16 else None,\n",
    "            sharding_strategy=fsdp_config.sharding_strategy,\n",
    "            device_id=torch.cuda.current_device(),\n",
    "            limit_all_gathers=False,\n",
    "        )\n",
    "        if fsdp_config.fsdp_activation_checkpointing:\n",
    "            policies.apply_fsdp_checkpointing(model)\n",
    "    elif not train_config.quantization and not train_config.enable_fsdp:\n",
    "        model.to(\"cuda\")\n",
    "\n",
    "    dataset_config = generate_dataset_config(train_config, kwargs)\n",
    "    \n",
    "     # Load and preprocess the dataset for training and validation\n",
    "    dataset_train = get_preprocessed_dataset(\n",
    "        tokenizer,\n",
    "        dataset_config,\n",
    "        split=\"train\",\n",
    "    )\n",
    "    \n",
    "    if not train_config.enable_fsdp or rank == 0:\n",
    "        print(f\"--> Training Set Length = {len(dataset_train)}\")\n",
    "\n",
    "    dataset_val = get_preprocessed_dataset(\n",
    "        tokenizer,\n",
    "        dataset_config,\n",
    "        split=\"test\",\n",
    "    )\n",
    "    if not train_config.enable_fsdp or rank == 0:\n",
    "            print(f\"--> Validation Set Length = {len(dataset_val)}\")\n",
    "\n",
    "    train_sampler = None\n",
    "    val_sampler = None\n",
    "    if train_config.enable_fsdp:\n",
    "        train_sampler = DistributedSampler(\n",
    "            dataset_train,\n",
    "            rank=dist.get_rank(),\n",
    "            num_replicas=dist.get_world_size(),\n",
    "            shuffle=True,\n",
    "        )\n",
    "        if train_config.run_validation:\n",
    "            val_sampler = DistributedSampler(\n",
    "                dataset_val,\n",
    "                rank=dist.get_rank(),\n",
    "                num_replicas=dist.get_world_size(),\n",
    "            )\n",
    "        \n",
    "    # Create DataLoaders for the training and validation dataset\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=train_config.batch_size_training,\n",
    "        num_workers=train_config.num_workers_dataloader,\n",
    "        pin_memory=True,\n",
    "        sampler=train_sampler if train_sampler else None,\n",
    "        drop_last=True,\n",
    "        collate_fn=default_data_collator,\n",
    "    )\n",
    "\n",
    "    if train_config.run_validation:\n",
    "        eval_dataloader = torch.utils.data.DataLoader(\n",
    "            dataset_val,\n",
    "            batch_size=train_config.val_batch_size,\n",
    "            num_workers=train_config.num_workers_dataloader,\n",
    "            pin_memory=True,\n",
    "            sampler=val_sampler if val_sampler else None,\n",
    "            drop_last=True,\n",
    "            collate_fn=default_data_collator,\n",
    "        )\n",
    "        \n",
    "    # Initialize the optimizer and learning rate scheduler\n",
    "    if fsdp_config.pure_bf16 and fsdp_config.optimizer == \"anyprecision\":\n",
    "        optimizer = AnyPrecisionAdamW(\n",
    "            model.parameters(),\n",
    "            lr=train_config.lr,\n",
    "            momentum_dtype=torch.bfloat16,\n",
    "            variance_dtype=torch.bfloat16,\n",
    "            use_kahan_summation=False,\n",
    "        )\n",
    "    else:\n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=train_config.lr,\n",
    "            weight_decay=0.0,\n",
    "        )\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=train_config.gamma)\n",
    "\n",
    "    # Start the training process\n",
    "    results = train(\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        eval_dataloader, \n",
    "        tokenizer,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        gradient_accumulation_steps,\n",
    "        train_config,\n",
    "        fsdp_config if train_config.enable_fsdp else None,\n",
    "        local_rank if train_config.enable_fsdp else None,\n",
    "        rank if train_config.enable_fsdp else None,\n",
    "    )\n",
    "    if not train_config.enable_fsdp or rank==0:\n",
    "        [print(f'Key: {k}, Value: {v}') for k, v in results.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(main)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
